{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis of Restarent Reviews"
      ],
      "metadata": {
        "id": "J4VX5QaXIRS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libraries\n"
      ],
      "metadata": {
        "id": "4VCFa3IYH75s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPZPy3aXFq5G",
        "outputId": "231edd3d-3588-4913-ae13-6d0044209fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import numpy as np       #linear algebra\n",
        "import pandas as pd      #data processing ,csv file I/o\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  #connecting google drive with google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "x_2AfYzfIg4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare Data"
      ],
      "metadata": {
        "id": "ID3VVQgpIq5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Naveen Collab/Restaurant_Reviews.tsv',delimiter='\\t', quoting=3)"
      ],
      "metadata": {
        "id": "1Dyght7_IegZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "dbf46d1a-cb8c-4189-bb23-f223365b2823"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Vickey Collab/Restaurant_Reviews.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-47542529b78a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Vickey Collab/Restaurant_Reviews.tsv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Vickey Collab/Restaurant_Reviews.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "_lQUbBZyJfAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "fwJfaKBvKUMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "LcR4u6vlLCiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info"
      ],
      "metadata": {
        "id": "5jINZ8fCLPoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "nD3YNJ9LLcmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing essential libraries for performing natural language processing on restaurant review dataset\n",
        "\n",
        "import nltk              #natural language toolkit,is a python package that you can use for NLP\n",
        "import re            #python has inbuilt packagr called re, which can use to work with regular expressions\n",
        "nltk.download('stopwords')   #the NLTK corpus is a massive dump of all kinds of natural language data sets that are definently worth taking a look at\n",
        "from nltk.corpus import stopwords    #A stop is a commonly used word (such as \"the\",\"a\",\"an\",\"in\") that a search engine has been programmed to ignore\n",
        "from nltk.stem.porter import PorterStemmer   # Porters stemmer it is a stemmer which is mainly known for data mining and information retrival"
      ],
      "metadata": {
        "id": "zneIIhY4LXsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the reviews\n",
        "\n",
        "corpus = []\n",
        "for i in range(0,1000):\n",
        "\n",
        "  #1000 records\n",
        "  #cleaning special character from the reviews\n",
        "  #the re.sub() method performs global search and global replace on the given data\n",
        "    review = re.sub(pattern='[^a-zA-Z]', repl=' ', string=str(df['Review'][i]))\n",
        "\n",
        "    #convering the entire review into lowercase\n",
        "    review = review.lower()\n",
        "\n",
        "    #Tokenizing the review by words\n",
        "    review_words = review.split()\n",
        "\n",
        "    #removing the stop words\n",
        "    review_words = [word for word in review_words if not word in set(stopwords.words('english'))]    #stopwords are the english wordswhich doesn't add much meaning to a sentence\n",
        "\n",
        "      #streaming the stop words\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review_words]\n",
        "\n",
        "    #joining the stemmed words\n",
        "    review = ' '.join(review)\n",
        "\n",
        "   #creating a corpus\n",
        "    corpus.append(review)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ODwenL9jNv4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0:1500]"
      ],
      "metadata": {
        "id": "HCciMPARSIHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the bags of words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=1500)  #max_features = 1500\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = df.iloc[:,1].values"
      ],
      "metadata": {
        "id": "ksfzsuF8bsyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split Data - (1000,1500)**\n",
        "in this step, we are going to split data in two parts (training and testing),so that we can train our model on training dataset and test its accuracy on unseen test data"
      ],
      "metadata": {
        "id": "Dv-lOqBud2dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split   #used to test the data into training data and test data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state = 0)\n",
        "\n",
        "#random_state simply sets seed to the random generator, so that your train-test splits are always deterministic. if you dont set seed, it is difficult to each time\n",
        "\n",
        "#Training (1000,1500)\n",
        "#1000 * 80/100 = 800\n",
        "#1000 * 20/100 = 200\n"
      ],
      "metadata": {
        "id": "TSV27_bpdVfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "id": "6gMw38ceft2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway Points**\n",
        "\n",
        "\n",
        "*   Usually we take more and more dayta in training so its easy for the model to learn with more data\n",
        "\n"
      ],
      "metadata": {
        "id": "sFP7UipMg1qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "As we done with preprocessing part, it\n",
        "is time to train our model.\n",
        "\n",
        "**Multinomial Naive Bayes** - The algorithm is a probabilistic learning method that is mostly Used in Natural Language Processing(NLP).The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article"
      ],
      "metadata": {
        "id": "tzZHOJWVhdQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multinomial NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=0.1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "id": "iwTE-DVSBo-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the confusion matrix\n",
        "\n",
        "import matplotlib.pyplot as plt   #matplotlib is a low level graph plotting library in python that servs as a visualization utility\n",
        "import seaborn as sns    #seaborn is python data visualization library based on matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(cm,annot=True,cmap=\"YlGnBu\",xticklabels=[\"Negative\",\"positive\"],yticklabels=['Negative','Positive'])\n",
        "plt.xlabel('Predicted values')\n",
        "plt.ylabel('Actual values')"
      ],
      "metadata": {
        "id": "1rtLjSgMnplO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bernoulli NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB(alpha=0.8)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "id": "r62VqOSvBuJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn import linear_model\n",
        "classifier = linear_model.LogisticRegression(C=1.5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "id": "ZjuzdM_wBxDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions for the reviews"
      ],
      "metadata": {
        "id": "O1TA1MC1s3TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(sample_review):\n",
        "  sample_review = re.sub(pattern = '[^a-zA-Z]',repl=' ',string = sample_review)\n",
        "  sample_review =sample_review.lower()\n",
        "  sample_review_words = sample_review.split()\n",
        "  sample_review_words = [word for word in sample_review_words if not word in set(stopwords.words('english'))]\n",
        "  ps=PorterStemmer()\n",
        "  final_review = [ps.stem(word) for word in sample_review_words]\n",
        "  final_review = ' '.join(final_review)\n",
        "\n",
        "  temp = cv.transform([final_review]).toarray()\n",
        "  return classifier.predict(temp)\n"
      ],
      "metadata": {
        "id": "aOdm67xbsvYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicted values\n",
        "sample_review = 'The food is very very good'\n",
        "\n",
        "if predict_sentiment(sample_review):\n",
        "  print('This is a POSITIVE review')\n",
        "else:\n",
        "  print('This is Negative review!')"
      ],
      "metadata": {
        "id": "F0wluYjtu0cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicted values\n",
        "sample_review = 'The food pretty bad and the service was very slow'\n",
        "\n",
        "if predict_sentiment(sample_review):\n",
        "  print('This is a POSITIVE review')\n",
        "else:\n",
        "  print('This is Negative review!')"
      ],
      "metadata": {
        "id": "g8uJVCeEv7NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicted values\n",
        "sample_review = 'The food was absolutely wonderful, from preparation to presentation,very pleasing.'\n",
        "\n",
        "if predict_sentiment(sample_review):\n",
        "  print('This is a POSITIVE review')\n",
        "else:\n",
        "  print('This is Negative review!')"
      ],
      "metadata": {
        "id": "tm9koCLywLBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}